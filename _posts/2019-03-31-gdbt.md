---
layout : post
title: Gradirnt Boosting Decision Treeの勉強をした。
tags: [machine-learning, math]
---

# 動機

Kaggleなどで話題となっているLightGBMなどのことを知るためにまず、勾配ブースティング決定木の勉強をした。

$$
\DeclareMathOperator{\argmin}{arg\,min}
$$

## 決定木とは

決定木では特徴量 $x$ を元に排反な $J$ 個の分割領域 $\\{R_j\\}_{j=1}^J$ を構成し、

$x \in R_j$ のとき、予測値 $f(x)=\gamma_j$ を返す。

これをまとめて表現すると、$\Theta := \\{R_j, \gamma_j \\}_{j=1}^J$ として、

$$
f(x) = T(x; ) := \sum_{j=1}^J \gamma_j I(x\in R_j)
$$

とかける。

## ブースティング木

ブースティング木とは決定木をたくさん用意した予測モデルです。つまり

$$
f_M(x) = \sum_{m=1}^M T(x;\Theta_m)
$$

で予測するようなモデルです。[Adaboostの記事](https://maea2.github.io/adaboost) でやったように $f_M$ を逐次最適化してみましょう。

つまり、各 $\hat{\Theta}_m$ を推定するときに

$$
\hat{\Theta}_m = \underset{\Theta_m}{\argmin} \sum_{i=1}^N L(y_i, f_{m-1}(x )+ T(x;\Theta_m))
$$

で推定してやろうということです。

## 最急降下法

いま、関数　$f$ を使って予測したときの 損失関数の　$x$　についての条件付き期待値

$$
\Phi(x, f) = \mathbb{E}_{x,y} \left[L(y, f(x))| x\right]
$$

がわかっているとします。

\Phi(f) を最小にするような予測関数 $f^\ast(x):=\argmin_f \Phi(f)$ がいま得たいものです。


これを、

$$
f_M(x)=\sum_{m=1}^M f_m(x)
$$

の形で得ることを考えます。$f_1, f_2, f_3$ と少しづつ $f_i$ を増やして近似していくというイメージです。

 $f_i$ 一つ一つが弱学習器だと思ってください。

いま、$f_1,\ldots,f_{m-1}$ まで得られているとします。

いま、最急降下法を使って $f_m$ を計算することを考えます。

$$
g_m(x) = \left.\frac{\partial \Phi(x^\prime, f_{m-1})}{\partial f(x^\prime)}\right|_{x^\prime=x}
$$

と置くと、最急降下法によってもとまる $f_m$ は

$$
f_m(x) = f_{m-1}(x) - \rho_m g_m(x)
$$

となります。$\rho_m$ は $\Phi(x, f_m)$ が最小となるようにとります。

## 勾配ブースティングとは

実際は $(x,y)$ の分布は未知であり、有限のサンプルデータ $\\{(x_i,y_i)\\}$ しか持っていません。


なので、$g_m(x)$ の値は訓練データ点のところしかわかりません。よって、$\hat{\Theta}_m$ を推定するような問題を考えるとき、この方法は直接は使えません。

次善の策を考えます。

つまり有限個の勾配の値 $\\{g_m(x_i)\\}_{i=1}^N$ と $\\{T(x_i;\Theta)\\}$ が訓練データ点においてだけでも近くなるように


$$
\tilde{\Theta}_m :=\underset{\Theta}{\argmin}\sum_{i=1}^N (-g_m(x_i)-T(x_i;\Theta))^2
$$

で定めるという策です。

## 勾配ブースティング決定木

以上を踏まえて勾配ブースティング決定木の最適化アルゴリズムは次のようになります。

(b)の部分が勾配情報との近くなるようにしている部分です。

![](/images/gbdt.png)
**勾配ブースティング決定木のアルゴリズム (カステラ本のp361より引用)**

## 参考文献

- [Greedy function approximation a gradient boosting machine](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)

- [The Elements of
Statistical Learning](http://web.stanford.edu/~hastie/ElemStatLearn/)




I"<h1 id="ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³">ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³</h1>

<p>è¿‘å¹´ç”¨ã„ã‚‰ã‚Œã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§ã¯ãƒ‡ãƒ¼ã‚¿æ•°ã«å¯¾ã—ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒéå¸¸ã«å¤šã„çŠ¶æ³ãŒå¤šã„ã€‚ã“ã‚Œã¯æ—¢å­˜ã®å­¦ç¿’ç†è«–ã®æ çµ„ã¿ã¨ã¯ç›¸åã™ã‚‹ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒéå¸¸ã«å¤šã„ãƒ¢ãƒ‡ãƒ«ã®ã“ã¨ã‚’over-parameterized modelã¨ã„ã†ã€‚</p>

<h1 id="2å±¤ã®å ´åˆã®çµæœ">2å±¤ã®å ´åˆã®çµæœ</h1>

<p>[1]ã§ã¯2å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã§äºŒä¹—æå¤±ã‚’ç”¨ã„ãŸå ´åˆã«ååˆ†ã«ãƒ¦ãƒ‹ãƒƒãƒˆæ•°ã§$m$ãŒå¤§ãã‘ã‚Œã°ã€å‹¾é…æ³•ã«ã‚ˆã‚‹å­¦ç¿’ã«ã‚ˆã£ã¦æå¤±ã‚’é™ã‚Šãªãå°ã•ãã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚</p>

<h2 id="1ã®è«–æ–‡ã®è¨­å®š">[1]ã®è«–æ–‡ã®è¨­å®š</h2>

<p>2å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯æ¬¡å¼ã§å®šç¾©ã•ã‚Œã‚‹ã€‚ã“ã“ã§$\sigma$ã¯ReLUã§ã‚ã‚‹ã€‚</p>

\[f(\boldsymbol{x};\boldsymbol{W},\boldsymbol{a})=\frac{1}{\sqrt{m}}\sum_{r=1}^ma_i\sigma(\boldsymbol{w}_r^\top\boldsymbol{x})\]

<p>æ¬¡ã®äºŒä¹—æå¤±ã‚’æœ€å°åŒ–ã™ã‚‹ã€‚</p>

\[L(\boldsymbol{W}, \boldsymbol{a}) = \sum_{i=1}^n \frac{1}{2}\left(y_i-f(\boldsymbol{x}_i;\boldsymbol{W}, \boldsymbol{a})\right)^2\]

<p>å­¦ç¿’ã¯å‹¾é…æ³•ã§è¡Œã†ã€‚</p>

\[\boldsymbol{W}(k+1) = \boldsymbol{W}(k)-\eta\frac{\partial L(\boldsymbol{W}(k), \boldsymbol{a})}{\partial\boldsymbol{W}(k)}\]

<h2 id="neural-tangent-kernel">Neural Tangent Kernel</h2>

<p>ã„ã¾ã€æå¤±ã‚’$L(\boldsymbol{W},\boldsymbol{a})=\sum_{i=1}^n \ell_i(f_{\boldsymbol{W}}(\boldsymbol{x}_i);\boldsymbol{a})$ã§å®šã‚ã¦ãŠãã€‚$\boldsymbol{a}$ã¯å›ºå®šã™ã‚‹ã®ãŒãƒã‚¤ãƒ³ãƒˆã«ãªã£ã¦ã„ã‚‹ã€‚Neural Tangent Kernelã¯2å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å‹¾é…æµã‚’è€ƒãˆã‚‹ã¨å‡ºã¦ãã‚‹é‡
ã§è§£æã®éš›ã«é‡è¦ãªå½¹å‰²ã‚’æœãŸã™ã€‚å­¦ç¿’ã®ã‚¹ãƒ†ãƒƒãƒ—å¹…ã‚’é™ã‚Šãªãå°ã•ãã™ã‚‹ã¨ã€æ¬¡ã®å‹¾é…æµã®å¼ãŒã§ã¦ãã‚‹ã€‚</p>

\[\frac{\mathrm{d}\boldsymbol{w}_r}{\mathrm{d}t}=-\nabla_{\boldsymbol{w}_r} L(\boldsymbol{W},\boldsymbol{a})\]

<p>ã“ã“ã§ã€</p>

\[\begin{align}
\frac{\mathrm{d}\boldsymbol{w}_r}{\mathrm{d}t}&amp;=-\nabla_{\boldsymbol{w}_r} L(\boldsymbol{W},\boldsymbol{a})\\
&amp;=-\sum_{i=1}^n \nabla_{\boldsymbol{w}_r} \ell_i(f_{\boldsymbol{W}}(\boldsymbol{x}_i);\boldsymbol{a})\\
&amp;=-\sum_{i=1}^n \frac{\mathrm{d}\ell_i(f_{\boldsymbol{W}})}{\mathrm{d}f_{\boldsymbol{W}}}a_r\nabla_{\boldsymbol{w}_r}\sigma(\boldsymbol{w}_r^\top\boldsymbol{x}_i)\\

\end{align}\]

<p>ã—ãŸãŒã£ã¦ã€</p>

\[\begin{align}
\frac{\mathrm{d} f_{\boldsymbol{W}}(\boldsymbol{x})}{\mathrm{d}t}=&amp;\sum_{r=1}^m\nabla_{\boldsymbol{w}_r}^\top f_{\boldsymbol{W}}\frac{\mathrm{d}\boldsymbol{w}_r}{\mathrm{d}t}\\
=&amp;\sum_{r=1}^m\nabla_{\boldsymbol{w}_r}^\top f_{\boldsymbol{W}}\left(-\sum_{i=1}^n \frac{\mathrm{d}\ell_i(f_{\boldsymbol{W}})}{\mathrm{d}f_{\boldsymbol{W}}}\nabla_{\boldsymbol{w}_r}\sigma(\boldsymbol{w}_r^\top\boldsymbol{x}_i)\right)\\
=&amp;\sum_{r=1}^m\nabla_{\boldsymbol{w}_r}^\top f_{\boldsymbol{W}}(\boldsymbol{x})\left(-\sum_{i=1}^n \frac{\mathrm{d}\ell_i(f_{\boldsymbol{W}})}{\mathrm{d}f_{\boldsymbol{W}}}\nabla_{\boldsymbol{w}_r}\sigma(\boldsymbol{w}_r^\top\boldsymbol{x}_i)\right)\\
=&amp;\sum_{r=1}^ma_r\nabla_{\boldsymbol{w}_r}^\top\sigma(\boldsymbol{w}_r^\top\boldsymbol{x})\left(-\sum_{i=1}^n \frac{\mathrm{d}\ell_i(f_{\boldsymbol{W}})}{\mathrm{d}f_{\boldsymbol{W}}}\nabla_{w_r}\sigma(\boldsymbol{w}_r^\top\boldsymbol{x}_i)\right)\\
=&amp;-\sum_{i=1}^n\frac{\mathrm{d}\ell_i(f_{\boldsymbol{W}})}{\mathrm{d}f_{\boldsymbol{W}}}\underbrace{\sum_{r=1}^ma_r\nabla_{\boldsymbol{w}_r}^\top\sigma(\boldsymbol{w}_r^\top\boldsymbol{x}) \nabla_{w_r}\sigma(\boldsymbol{w}_r^\top\boldsymbol{x}_i)}_{=:k_{\boldsymbol{W}}(\boldsymbol{x},\boldsymbol{x}_i)}
\end{align}\]

<p>$k_{\boldsymbol{W}}$ã‚’Neural Tangent Kernelã¨ã„ã†ã€‚å¤šå±¤ã®å ´åˆã‚‚ä¸­é–“å±¤ã®å‡ºåŠ›ã‚’è€ƒãˆã‚Œã°åŒæ§˜ã®æ‰‹é †ã§æ±‚ã‚ã‚‹ã“ã¨ãŒã§ãã‚‹ã¯ãšã€‚</p>

<h2 id="1ã®è«–æ–‡ã®ã‚­ãƒ¢">[1]ã®è«–æ–‡ã®ã‚­ãƒ¢</h2>

<p>ä»–ã®è«–æ–‡ã§ã‚‚ç¹°ã‚Šè¿”ã—å‡ºã¦ãã‚‹ãŒã€æ¬¡ã®Gram matrix $\boldsymbol{H}^\infty$ãŒéå¸¸ã«é‡è¦ãªå½¹å‰²ã‚’æœãŸã™ã€‚</p>

\[\boldsymbol{H}^\infty_{i,j} = \mathbb{E}_{\boldsymbol{w}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I})}\left[\boldsymbol{x}_i^\top\boldsymbol{x}_j\mathbb{I}_{\left\{\boldsymbol{w}^\top\boldsymbol{x}_i\geq 0\text{ and } \boldsymbol{w}^\top\boldsymbol{x}_j\geq 0\right\}}\right]\]

<h2 id="è«–æ–‡ã®ä¸»å¼µ">è«–æ–‡ã®ä¸»å¼µ</h2>

<div class="box">

$\boldsymbol{w}_r\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I})$
, $a_r\sim\mathrm{unif}(0,1)$ã§åˆæœŸåŒ–ã•ã‚ŒãŸ2å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¯ãƒ¦ãƒ‹ãƒƒãƒˆæ•°ã‚’$m=\Omega\left(n^6/\lambda^4\delta^3\right)$ã«ã—ã€å­¦ç¿’ç‡$\eta=\Omega(\lambda_0/n^2)$ã‚’ã¨ã™ã‚‹ã¨ç¢ºç‡$1-\delta$ã§æ¬¡å¼ãŒæˆã‚Šç«‹ã¤ã€‚ã“ã“ã§$\boldsymbol{u}_i(k)=f(\boldsymbol{x}_i;\boldsymbol{W}(k),\boldsymbol{a})(i=1,2,\dots,n$ã§ã‚ã‚‹ã€‚

$$
\|\boldsymbol{u}(k)-\boldsymbol{y}\|_2^2\leq \left(1-\frac{\eta\lambda_0}{2}\right)^k\|\boldsymbol{u}(0)-\boldsymbol{y}\|_2^2
$$

</div>

<h1 id="æ±åŒ–ã«ã¤ã„ã¦">æ±åŒ–ã«ã¤ã„ã¦</h1>

<h1 id="å‚è€ƒæ–‡çŒ®">å‚è€ƒæ–‡çŒ®</h1>

<ul>
  <li>[1]ã§ã¯
 <a href="https://arxiv.org/abs/1810.02054">GRADIENTDESCENTPROVABLYO PTIMIZES OVER-PARAMETERIZEDNEURALN ETWORKS</a></li>
  <li>[2] <a href="https://arxiv.org/abs/1901.08584">Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks</a></li>
  <li>[3] <a href="http://arxiv.org/abs/1811.03962">A Convergence Theory for Deep Learning via Over-Parameterization</a></li>
  <li>[4] <a href="http://arxiv.org/abs/1811.03804">Gradient Descent Finds Global Minima of Deep Neural Networks</a></li>
  <li>[5] <a href="https://arxiv.org/abs/1811.04918.">Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers</a></li>
</ul>
:ET
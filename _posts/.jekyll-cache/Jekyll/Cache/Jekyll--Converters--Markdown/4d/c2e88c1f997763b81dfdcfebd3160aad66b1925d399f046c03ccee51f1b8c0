I"	<h2 id="要旨">要旨</h2>

<p>一般に強化学習を実世界に応用するとき、環境の情報は十分には得られない(ゲームのルールがわからない)。また環境から報酬を得ること自体のコストが高い(無制限に試行できない)。応用する問題によって適切なエージェントは異なるので、複数のエージェントから環境に最も適応した個体を選ぶバンディット問題とみなして最適なエージェントを選ぶことを提案している。その際、情報理論的には次の状態の分布とモデルのパラメータの分布の相互情報量が最大化となるもの(もっとも驚きが大きいものを)選ぶのが良い。ただ、モデルのパラメータの事後分布を直接計算することは難しいので、そこを変分近似した。</p>

<h2 id="bandit問題">Bandit問題</h2>

<p>$K$ 台のスロットマシンがそれぞれ期待値 $\mu_i$ (未知) に設定されているときに、最大の期待値 $\mu^\ast:=\max_{i} \mu_i$ を持つスロットマシンを探索するという問題です。なるべく探索回数を少なくして，得られる報酬も最大化するのが目標です。</p>

<p>スロットマシンの設定によって名前がついている。[2]にわかりやすくかいてあって参考になりました。</p>

<ul>
  <li>確率的Bandit</li>
  <li>敵対的Bandit</li>
</ul>

<h2 id="強化学習としてのbandit問題">強化学習としてのBandit問題</h2>

<p>強化学習で扱うのは行動によって環境から報酬が得られるという状況において、何度か試行することで将来の報酬を最大化するような行動を探そうという設定の問題です。Bandit問題において毎回スロットマシンを引くことを行動とみなすと、Bandit問題は強化学習の問題とみなすことができます。</p>

<h2 id="本論文の貢献">本論文の貢献</h2>

<p>おそらく最適なエージェントの選択をバンディットとみなした点(？)</p>

<h2 id="参考文献">参考文献</h2>

<ul>
  <li>[1] <a href="https://arxiv.org/abs/1902.03657">A Bandit Framework for Optimal Selection of Reinforcement Learning Agents</a></li>
  <li>[2] <a href="http://ibisml.org/archive/ibis2014/ibis2014_bandit.pdf">多腕バンディット問題の
理論とアルゴリズム</a></li>
</ul>
:ET